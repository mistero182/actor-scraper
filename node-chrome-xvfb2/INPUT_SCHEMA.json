{
    "title": "Puppeteer Scraper Input",
    "description": "Use the following form to configure Puppeteer Scraper. The Start URLs and Page function are required and all other fields are optional. To learn more about the different options, click on their title or on the nearby question marks. For details about the Page function visit the actor's README in the ACTOR INFO tab.",
    "type": "object",
    "schemaVersion": 1,
    "properties": {
        "startUrls": {
            "sectionCaption": "Basic configuration",
            "title": "Start URLs",
            "type": "array",
            "description": "URLs to start with",
            "prefill": [
                {
                    "url": "https://apify.com"
                }
            ],
            "editor": "requestListSources"
        },
        "pageFunction": {
            "title": "Page function",
            "type": "string",
            "description": "Function executed for each request",
            "prefill": "async function pageFunction(context) {\n    const { page, request, log } = context;\n    const title = await page.title();\n    log.info(`URL: ${request.url} TITLE: ${title}`);\n    return {\n        url: request.url,\n        title\n    };\n}",
            "editor": "javascript"
        },
        "preNavigationHooks": {
            "title": "Pre-navigation hooks",
            "type": "string",
            "description": "Async functions that are sequentially evaluated before the navigation. Good for setting additional cookies or browser properties before navigation. The function accepts two parameters, `crawlingContext` and `gotoOptions`, which are passed to the `page.goto()` function the crawler calls to navigate.",
            "prefill": "// We need to return array of (possibly async) functions here.\n// The functions accept two arguments: the \"crawlingContext\" object\n// and \"gotoOptions\".\n[\n    async (crawlingContext, gotoOptions) => {\n        const { page } = crawlingContext;\n        // ...\n    },\n]",
            "editor": "javascript"
        },
        "proxyConfiguration": {
            "sectionCaption": "Proxy and browser configuration",
            "title": "Proxy configuration",
            "type": "object",
            "description": "Choose to use no proxy, Apify Proxy, or provide custom proxy URLs.",
            "prefill": {
                "useApifyProxy": true
            },
            "default": {
                "useApifyProxy": false
            },
            "editor": "proxy"
        },
        "proxyRotation": {
            "title": "Proxy rotation",
            "type": "string",
            "description": "This property indicates the strategy of proxy rotation and can only be used in conjunction with Apify Proxy. The recommended setting automatically picks the best proxies from your available pool and rotates them evenly, discarding proxies that become blocked or unresponsive. If this strategy does not work for you for any reason, you may configure the scraper to either use a new proxy for each request, or to use one proxy as long as possible, until the proxy fails. IMPORTANT: This setting will only use your available Apify Proxy pool, so if you don't have enough proxies for a given task, no rotation setting will produce satisfactory results.",
            "default": "RECOMMENDED",
            "editor": "select",
            "enum": [
                "RECOMMENDED",
                "PER_REQUEST",
                "UNTIL_FAILURE"
            ],
            "enumTitles": [
                "Use recommended settings",
                "Rotate proxy after each request",
                "Use one proxy until failure"
            ]
        },
        "headless": {
            "title": "Headless browser",
            "type": "boolean",
            "description": "Eanble for headless browser mode, or disable to headful browser mode",
            "default": true,
            "groupCaption": "Browser masking",
            "groupDescription": "Settings that help mask as a real user and prevent scraper detection."
        },
        "useChrome": {
            "title": "Use Chrome",
            "type": "boolean",
            "description": "The scraper will use a real Chrome browser instead of a Chromium masking as Chrome. Using this option may help with bypassing certain anti-scraping protections, but risks that the scraper will be unstable or not work at all.",
            "default": false,
            "groupCaption": "Browser masking",
            "groupDescription": "Settings that help mask as a real user and prevent scraper detection."
        },
        "useStealth": {
            "title": "Use Stealth",
            "type": "boolean",
            "description": "The scraper will apply various browser emulation techniques to match a real user as closely as possible. This feature works best in conjunction with the Use Chrome option and also carries the risk of making the scraper unstable.",
            "default": false
        },
        "ignoreSslErrors": {
            "title": "Ignore SSL errors",
            "type": "boolean",
            "description": "Scraper will ignore SSL certificate errors.",
            "default": false,
            "groupCaption": "Security",
            "groupDescription": "Various options that help you disable browser security features such as HTTPS certificates and CORS."
        },
        "ignoreCorsAndCsp": {
            "title": "Ignore CORS and CSP",
            "type": "boolean",
            "description": "Scraper will ignore CSP (content security policy) and CORS (cross origin resource sharing) settings of visited pages and requested domains. This enables you to freely use XHR/Fetch to make HTTP requests from the scraper.",
            "default": false
        },
        "maxRequestRetries": {
            "title": "Max request retries",
            "type": "integer",
            "description": "Maximum number of times the request for the page will be retried in case of an error. Setting it to 0 means that the request will be attempted once and will not be retried if it fails.",
            "minimum": 0,
            "default": 3,
            "unit": "retries"
        },
        "maxConcurrency": {
            "title": "Max concurrency",
            "type": "integer",
            "description": "Defines how many pages can be processed by the scraper in parallel. The scraper automatically increases and decreases concurrency based on available system resources. Use this option to set a hard limit.",
            "minimum": 1,
            "default": 50
        },
        "pageLoadTimeoutSecs": {
            "title": "Page load timeout",
            "type": "integer",
            "description": "Maximum time the scraper will allow a web page to load in seconds.",
            "minimum": 1,
            "default": 60,
            "unit": "secs"
        },
        "pageFunctionTimeoutSecs": {
            "title": "Page function timeout",
            "type": "integer",
            "description": "Maximum time the scraper will wait for the page function to execute in seconds.",
            "minimum": 1,
            "default": 60,
            "unit": "secs"
        },
        "waitUntil": {
            "title": "Navigation wait until",
            "type": "array",
            "description": "The scraper will wait until the selected events are triggered in the page before executing the page function. Available events are <code>domcontentloaded</code>, <code>load</code>, <code>networkidle2</code> and <code>networkidle0</code>. <a href=\"https://pptr.dev/#?product=Puppeteer&show=api-pagegotourl-options\" target=\"_blank\">See Puppeteer docs</a>.",
            "default": [
                "networkidle2"
            ],
            "prefill": [
                "networkidle2"
            ],
            "editor": "json"
        },
        "userAgent": {
            "title": "OS & browser info visible for web site",
            "type": "string",
            "description": "Change the user-agent to bypass specific OS and browsers restrictions",
            "prefill": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82 Safari/537.36",
            "editor": "textfield"
        },
        "userAgentOS": {
            "sectionCaption": "UserAgent configuration",
            "title": "OS",
            "type": "string",
            "description": "Select an OS",
            "editor": "select",
            "default": "",
            "enum": ["", "Windows", "Mac OS", "Ubuntu", "Fedora", "Debian", "Android"],
            "enumTitles": ["Random", "Windows", "Mac OS", "Ubuntu", "Fedora", "Debian", "Android"]
        },
        "userAgentBrowser": {
            "title": "Browser",
            "type": "string",
            "description": "Select a Browser",
            "editor": "select",
            "default": "",
            "enum": ["", "Chrome", "Firefox", "Opera", "Chromium", "Edge", "Safari"],
            "enumTitles": ["Random", "Chrome", "Firefox", "Opera", "Chromium", "Edge", "Safari"]
        },
        "debugLog": {
            "sectionCaption": "Advanced configuration",
            "title": "Debug log",
            "type": "boolean",
            "description": "Debug messages will be included in the log. Use <code>context.log.debug('message')</code> to log your own debug messages.",
            "default": false,
            "groupCaption": "Enable logs",
            "groupDescription": "Logs settings"
        },
        "browserLog": {
            "title": "Browser log",
            "type": "boolean",
            "description": "Console messages from the Browser will be included in the log. This may result in the log being flooded by error messages, warnings and other messages of little value, especially with high concurrency.",
            "default": false
        }
    },
    "required": [
        "startUrls"
    ]
}